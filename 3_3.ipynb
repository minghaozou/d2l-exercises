{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Regression Data"
      ],
      "metadata": {
        "id": "cAYB4_JDmews"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jedi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee_rGlRopUDD",
        "outputId": "7bf2ac7c-5edb-4059-bcd9-0c9e202bfdba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi) (0.8.4)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep pip/tools fresh\n",
        "!pip -q install -U pip setuptools wheel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDhFwQm5pCMX",
        "outputId": "92bc9d77-ce05-45f2-8222-d09ed8501b39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.5/1.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install d2l but SKIP its pinned (old) deps\n",
        "!pip -q install d2l==1.0.3 --no-deps"
      ],
      "metadata": {
        "id": "PJVtfiUPpCEo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dA1MrzjLmdfQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the Dataset"
      ],
      "metadata": {
        "id": "u2hKULEXqEdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SyntheticRegressionData(d2l.DataModule):\n",
        "    \"\"\"Synthecti data for linear regression\"\"\"\n",
        "    def __init__(self, w, b, noise = 0.01, num_train = 1000, num_val = 1000,\n",
        "                 batch_size = 32):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        n = num_train + num_val\n",
        "        self.X = torch.randn(n, len(w))\n",
        "        noise = torch.randn(n, 1) * noise\n",
        "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise"
      ],
      "metadata": {
        "id": "toBJguQ3qGiE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = SyntheticRegressionData(w = torch.tensor([2, -3.4]), b = 4.2)"
      ],
      "metadata": {
        "id": "PU3v4CuiGPpP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('features', data.X[0], '\\nlabel:', data.y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG74f_7pGR8s",
        "outputId": "a21da7f1-7a4f-40ac-ad85-92e28982c7de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features tensor([ 1.1986, -1.9502]) \n",
            "label: tensor([13.2176])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the Dataset"
      ],
      "metadata": {
        "id": "Dcq7qERWP7MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(SyntheticRegressionData)\n",
        "def get_dataloader(self, train):\n",
        "    if train:\n",
        "        indices = list(range(0, self.num_train))\n",
        "        # The examples are read in random order\n",
        "        random.shuffle(indices)\n",
        "    else:\n",
        "        indices = list(range(self.num_train, self.num_train + self.num_val))\n",
        "    for i in range(0, len(indices), self.batch_size):\n",
        "        batch_indices = torch.tensor(indices[i:i + self.batch_size])\n",
        "        yield self.X[batch_indices], self.y[batch_indices]"
      ],
      "metadata": {
        "id": "8RCU4Gr2GZtX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = next(iter(data.train_dataloader()))\n",
        "print('X shape:', X.shape, '\\ny shape:', y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgAIdyJobrKC",
        "outputId": "164e5290-c076-4ff4-f5ed-a97b00f81896"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: torch.Size([32, 2]) \n",
            "y shape: torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hand-rolled generator is inefficient."
      ],
      "metadata": {
        "id": "dLWn4rJCbxEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concise Implementation of the Data Loader"
      ],
      "metadata": {
        "id": "MpL2jDvXb0iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(d2l.DataModule)\n",
        "def get_tensorloader(self, tensors, train, indices = slice(0, None)):\n",
        "    tensors = tuple(a[indices] for a in tensors)\n",
        "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
        "    return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
        "                                        shuffle = train)"
      ],
      "metadata": {
        "id": "mRAJcMCebutc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(SyntheticRegressionData)\n",
        "def get_dataloader(self, train):\n",
        "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
        "    return self.get_tensorloader((self.X, self.y), train, i)"
      ],
      "metadata": {
        "id": "XRJlkpdskkQP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = next(iter(data.train_dataloader()))\n",
        "print('X shape:', X.shape, '\\ny shape:', y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYm3Kkppkr6j",
        "outputId": "52478f0b-38fb-4bce-8b07-b04ca754ff63"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: torch.Size([32, 2]) \n",
            "y shape: torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data.train_dataloader())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91IutRPTksc8",
        "outputId": "0672702b-6937-444a-f2c4-588294c8e4ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "TeAOka4Fk6NF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.5.1"
      ],
      "metadata": {
        "id": "-ZLO8Wk6k78J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What will happen if the number of examples cannot be divided by the batch size. How would you change this behavior by specifying a different argument by using the framework's API?"
      ],
      "metadata": {
        "id": "HsPKqGEWlHg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If $N$ (number of examples) isn’t a multiple of $B$ (batch size), the last batch is smaller by default.\n",
        "\n",
        "Example: $N = 1000, B = 32 \\implies$ you get 31 full batches of 32 and one final batch of 8.\n",
        "\n",
        "To change this behavior in PyTorch, pass `drop_last=True` to the `DataLoader` so the incomplete final batch is dropped (all batches have the same size):\n",
        "\n",
        "When to use which:\n",
        "\n",
        "*   Training: `drop_last=True` is handy if you need fixed batch size (e.g., BatchNorm, certain fused kernels, multi-GPU balance).\n",
        "*   Validation/Test: keep `drop_last=False` to evaluate all examples.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vf8ZieVWlhaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(d2l.DataModule)\n",
        "def get_tensorloader(self, tensors, train, indices = slice(0, None)):\n",
        "    tensors = tuple(a[indices] for a in tensors)\n",
        "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
        "    return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
        "                                        shuffle = train,\n",
        "                                       drop_last=train)"
      ],
      "metadata": {
        "id": "RnxKAdj0k7co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.5.2"
      ],
      "metadata": {
        "id": "MJdxFOZOm6MU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that we want to generate a huge dataset, where both the size of the parameter vector w and the number of examples num_examples are large.\n",
        "1. What happens if we cannot hold all data in memory?\n",
        "2. How would you shuffle the data if it is held on disk? Your task is to design an efficient algorithm that does not require too many random reads or writes. Hint: pseudorandom permutation generators allow you to design a reshuffle without the need to store the permutation table explicitly (Naor and Reingold, 1999)."
      ],
      "metadata": {
        "id": "V8v6QO9TSp71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Don’t preload everything” — stream minibatches instead\n",
        "\n",
        "Problem (in human terms):\n",
        "\n",
        "Your dataset is like a 1-terabyte photo album, but your computer’s memory (RAM) is only 16-GB. You can’t open the whole album at once.\n",
        "\n",
        "Fix:\n",
        "\n",
        "Open one page at a time: read just enough examples to make a minibatch, feed them to the model, then read the next minibatch. That’s streaming.\n",
        "\n",
        "Why this works:\n",
        "\n",
        "You only ever keep one minibatch (say 1–64 MB) in memory, not the whole dataset."
      ],
      "metadata": {
        "id": "nxi9VS51TyLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Store data in shards” — medium files, not one giant blob\n",
        "\n",
        "Problem:\n",
        "\n",
        "Disk is fast if you read sequentially, but slow if you jump around randomly. One giant file can be unwieldy; thousands of tiny files cause overhead.\n",
        "\n",
        "Fix:\n",
        "\n",
        "Split data into shards: many medium-sized files (e.g., 64–256 MB each).\n",
        "\n",
        "\n",
        "*   Big enough to read efficiently in long, sequential chunks.\n",
        "*   Small enough that you can reshuffle the order of shards each epoch (a cheap way to get randomness).\n",
        "\n",
        "\n",
        "Picture:\n",
        "\n",
        "Think of 100 labeled boxes instead of 1 huge crate or 10,000 envelopes. You can shuffle box order easily, and inside each box you grab items in order."
      ],
      "metadata": {
        "id": "zeDHtfyrT8I4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“If `w` is huge” — the parameters don’t fit easily\n",
        "\n",
        "Sometimes the model is the big thing. Examples:\n",
        "\n",
        "\n",
        "*   A gigantic embedding table for words/items/users (millions of rows).\n",
        "*   A very wide linear model (10M+ features).\n",
        "\n",
        "\n",
        "Here are practical tricks to still train:\n",
        "\n",
        "**Minibatch / online SGD**\n",
        "\n",
        "Key idea: You never need the whole dataset in memory to compute a gradient—only the current minibatch. So even with a huge dataset, training works the same: stream minibatches (from disk) and update `w` each step.\n",
        "\n",
        "**Sparse updates (only touch what you use)**\n",
        "\n",
        "When it helps: Features are “mostly zeros” (e.g., bag-of-words). Each sample only references a tiny subset of parameters.\n",
        "\n",
        "Trick: Use layers/ops that do indexed lookups and sparse updates (e.g., `nn.Embedding` / `EmbeddingBag`).\n",
        "\n",
        "\n",
        "*   The optimizer updates only the rows of w that were actually used in the minibatch.\n",
        "*   Memory traffic and compute are much smaller than touching all of `w`.\n",
        "\n",
        "Mental picture: You have a phone book with 10 million names (parameters), but for one call (minibatch) you look up and update only 500 of them."
      ],
      "metadata": {
        "id": "aJVA41fYVKGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The problem we’re solving**\n",
        "\n",
        "You have N records on disk. You want to read them in a new random order each epoch (to train well), without doing tons of slow random disk reads.\n",
        "\n",
        "If you just pick a random index for every next record, the disk has to jump all over the place → slow.\n",
        "\n",
        "**The fix: Block PRP (shuffle big chunks, read each chunk sequentially)**\n",
        "\n",
        "Instead of permuting records, permute blocks of records.\n",
        "\n",
        "\n",
        "\n",
        "1.   Pick a block size B (e.g., 64k records).\n",
        "2.   Then you have M = ceil(N / B) blocks, numbered 0..M-1:\n",
        "\n",
        "*   Block 0 holds records [0 .. B-1]\n",
        "*   Block 1 holds records [B .. 2B-1]\n",
        "*   ...\n",
        "\n",
        "3.   Use a PRP on the block IDs to get a new block order each epoch.\n",
        "4.   For each block (in that permuted order), read it sequentially (fast!), and optionally do a small in-RAM shuffle within the block for extra randomness.\n"
      ],
      "metadata": {
        "id": "6ul90rEhYqsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.5.3"
      ],
      "metadata": {
        "id": "H6tOCCiFcA19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a data generator that produces new data on the fly, every time the iterator is called."
      ],
      "metadata": {
        "id": "iuE6w2sCcEzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "class _OnTheFlyRegression(d2l.HyperParameters, IterableDataset):\n",
        "    \"\"\"Stream fresh samples: X ~ N(0, I), y = X·w + b + eps.\"\"\"\n",
        "    def __init__(self, w, b, noise = 0.01, num_train = 1000, num_val = 1000,\n",
        "                 train = False):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.w = torch.as_tensor(self.w, dtype = torch.float32).reshape(-1) # (d, )\n",
        "\n",
        "    def __iter__(self):\n",
        "        d = self.w.numel()\n",
        "        L = self.num_train if self.train else self.num_val  # <- pick train/val length\n",
        "        for _ in range(L):\n",
        "            X = torch.randn(d)\n",
        "            eps = torch.randn(1) * self.noise\n",
        "            y = (X @ self.w) + self.b + eps\n",
        "            yield X, y"
      ],
      "metadata": {
        "id": "cUxNb-vOcDim"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Every pass through the loop makes a brand-new `X` and noise `eps`.\n",
        "*   `y` is computed from those fresh values.\n",
        "*   `yield` returns that sample to the `DataLoader`, which stacks many such samples into a minibatch.\n",
        "\n",
        "So the on-the-fly generation is exactly the `torch.randn(...)` calls inside `__iter__`, executed each time you iterate.\n",
        "\n",
        "**Non-streaming(book's original)**\n",
        "No new calls to `torch.randn` during iteration.\n",
        "\n",
        "Iteration indexes into already-made tensors; you may shuffle order, but the values don’t change across epochs.\n",
        "\n",
        "**Streaming(on-the-fly)**\n",
        "\n",
        "Data is not stored up front. It’s generated during iteration:"
      ],
      "metadata": {
        "id": "xVHEXqHd4zOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SyntheticRegressionData(d2l.DataModule):\n",
        "    \"\"\"Synthetic data for linear regression (precomputed OR streaming).\"\"\"\n",
        "    def __init__(self, w, b, noise = 0.01, num_train = 1000, num_val = 1000,\n",
        "                 batch_size = 32, streaming = False):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        if not self.streaming:\n",
        "            n = num_train + num_val\n",
        "            self.X = torch.randn(n, len(w))\n",
        "            noise = torch.randn(n, 1) * noise\n",
        "            self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise"
      ],
      "metadata": {
        "id": "UWDtKozwMDIa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(d2l.DataModule)\n",
        "def get_tensorloader(self, tensors, train, indices = slice(0, None)):\n",
        "    tensors = tuple(a[indices] for a in tensors)\n",
        "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
        "    return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
        "                                        shuffle = train)"
      ],
      "metadata": {
        "id": "Rz_bY2tN19XX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch between precomputed (non-streaming) and on-the-fly (streaming)\n",
        "@d2l.add_to_class(SyntheticRegressionData)\n",
        "def get_dataloader(self, train: bool):\n",
        "    if getattr(self, \"streaming\", False):\n",
        "        ds = _OnTheFlyRegression(self.w, self.b, self.noise,\n",
        "                                 self.num_train, self.num_val, train)\n",
        "        return DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n",
        "    else:\n",
        "        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
        "        return self.get_tensorloader((self.X, self.y), train, i)"
      ],
      "metadata": {
        "id": "qptppuMwF1Ik"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What `getattr(self, \"streaming\", False)` means**\n",
        "\n",
        "`getattr(obj, name, default)` returns `obj.name` if it exists; otherwise it returns `default`.\n",
        "\n",
        "So here it reads: “If `self.streaming` exists and is truthy, use the streaming path; otherwise act as if it’s `False`.”\n",
        "\n",
        "This avoids `AttributeError` if some instance doesn’t have `self.streaming` (e.g., older code or an object created before we introduced the flag)."
      ],
      "metadata": {
        "id": "7ckmXNVLHJsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = SyntheticRegressionData(w=torch.tensor([2.0, -3.4]), b=4.2,\n",
        "                               noise=0.01, num_train=1000, num_val=1000,\n",
        "                               batch_size=32, streaming=True)\n",
        "X, y = next(iter(data.train_dataloader()))\n",
        "print(X.shape, y.shape)  # torch.Size([32, 2]) torch.Size([32, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSfqZEFWMTa1",
        "outputId": "a00fce38-3954-4a9e-eb0a-11c0cadac78d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2]) torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "X, y = next(iter(data.train_dataloader()))\n",
        "```\n",
        "\n",
        "does two things:\n",
        "\n",
        "\n",
        "*   `iter(data.train_dataloader())` creates a new iterator over the DataLoader (i.e., “start an epoch from the beginning”).\n",
        "*  ` next(...)` asks that iterator for the first minibatch, and you assign that batch to the local variables `X` and `y`.\n",
        "\n",
        "\n",
        "**Why you don’t see `data.X` / `data.y` “saved” anymore:**\n",
        "\n",
        "\n",
        "*   In your class you only create and store `self.X` and `self.y` when `streaming == False`.\n",
        "*   When `streaming == True`, you do not create those tensors. Instead, `train_dataloader()` builds an `_OnTheFlyRegression` dataset that generates fresh samples inside `__iter__` and yields them to the DataLoader. Nothing is stored in `self`—the batches are created on demand and returned to you, then discarded unless you keep them.\n",
        "\n",
        "So that line does “save” the batch—into the local Python variables `X` and `y`. It just doesn’t store the whole dataset on `data.X`/`data.y` because, in streaming mode, that dataset doesn’t exist."
      ],
      "metadata": {
        "id": "kPhmjDXkPiVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.5.4"
      ],
      "metadata": {
        "id": "_q0g8fQMQlHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How would you design a random data generator that generates the same data each time it is called?"
      ],
      "metadata": {
        "id": "F7Un0K0PQvRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "class OnTheFlyDeterministic(d2l.HyperParameters, IterableDataset):\n",
        "    \"\"\"Generates the SAME sequence every time you iterate.\"\"\"\n",
        "    def __init__(self, w, b, noise = 0.01, num_train = 1000, num_val = 1000,\n",
        "                 train = True, seed = 0):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.w = torch.as_tensor(self.w, dtype = torch.float32).reshape(-1) # (d, )\n",
        "\n",
        "    def __iter__(self):\n",
        "        d = self.w.numel()\n",
        "        L = self.num_train if self.train else self.num_val  # <- pick train/val length\n",
        "\n",
        "        g = torch.Generator()  #local RNG (doesn't touch global state)\n",
        "        g.manual_seed(self.seed) #<- key line: reset RNG each iteration\n",
        "\n",
        "        for _ in range(L):\n",
        "            X = torch.randn(d, generator = g)\n",
        "            eps = torch.randn(1, generator = g) * self.noise\n",
        "            y = (X @ self.w) + self.b + eps\n",
        "            yield X, y"
      ],
      "metadata": {
        "id": "O4t94gMAQg4o"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What’s an RNG?**\n",
        "\n",
        "RNG = Random Number Generator. In code it’s a tiny state machine that spits out a sequence of numbers that look random. It’s deterministic: if you start it from the same seed, it will produce the same sequence every time.\n",
        "\n",
        "In PyTorch there are two flavors:\n",
        "\n",
        "\n",
        "*   a global RNG (used when you call `torch.randn(...)` without specifying anything), and\n",
        "*   local RNG objects you create with `torch.Generator()` and pass explicitly (e.g., `torch.randn(..., generator=g)`).\n",
        "\n",
        "\n",
        "Using a local RNG isolates your randomness so other parts of your program can’t “consume” random numbers and change your sequence.\n",
        "\n",
        "\n",
        "```\n",
        "g = torch.Generator()          # local RNG (doesn't touch global state)\n",
        "g.manual_seed(self.seed)       # reset g's internal state to a fixed start\n",
        "```\n",
        "\n",
        "`torch.Generator()`\n",
        "\n",
        "Makes a new RNG object `g` with its own internal state. It’s separate from PyTorch’s global RNG. Nothing else can accidentally advance it unless they also use `g`.\n",
        "\n",
        "`g.manual_seed(self.seed)`\n",
        "\n",
        "Sets g’s starting point. From now on, every call that uses `generator=g` will draw the same sequence of “random” numbers each time you recreate and reseed `g `with the same seed. That’s what gives you repeatable data."
      ],
      "metadata": {
        "id": "A3BS_JXNWH-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = OnTheFlyDeterministic(w=torch.tensor([2.0, -3.4]), b=4.2,\n",
        "                               noise=0.01, num_train=1000, train = True,\n",
        "                           seed = 42)\n",
        "\n",
        "loader = DataLoader(ds, batch_size=32, shuffle=False)\n",
        "\n",
        "X1, y1 = next(iter(loader))\n",
        "X2, y2 = next(iter(loader))\n",
        "\n",
        "print(\"First batch == first batch from new iterator?\",\n",
        "      torch.equal(X1, X2), torch.equal(y1, y2))  # True, True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hMbzLfXYGYy",
        "outputId": "273c760b-668e-40c3-9379-97fe7aefff37"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First batch == first batch from new iterator? True True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`DataLoader` is acting as a batch maker + iterator factory wrapped around your dataset `ds`. Here’s what it does, step by step, in plain terms:\n",
        "\n",
        "\n",
        "1.   Starts an iteration when you ask for it\n",
        "\n",
        "When you call `iter(loader)` (or loop for `X, y in loader`:), the loader starts pulling samples from ds.\n",
        "\n",
        "2.   Pulls items from your dataset\n",
        "\n",
        "Because `ds` is an IterableDataset, `DataLoader` just calls `ds.__iter__()` and reads items in the order the dataset yields them.\n",
        "\n",
        "Your dataset yields per-sample pairs `(X, y)` with shapes `(d,)` and `(1,)`.\n",
        "\n",
        "3.   Groups samples into minibatches of 32\n",
        "\n",
        "It collects 32 such samples at a time (that’s `batch_size=32`).\n",
        "\n",
        "4.   Stacks them into batched tensors (the “collate” step)\n",
        "\n",
        "The default collate function turns a list of 32 tuples into a tuple of stacked tensors:\n",
        "\n",
        "`X_batch` becomes shape `(32, d)`\n",
        "\n",
        "`y_batch` becomes shape `(32, 1)`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HslEVvzXZEuW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_V1mqtQZ5YE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}